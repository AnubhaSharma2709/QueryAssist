{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import streamlit as st \n",
    "import pickle \n",
    "import time\n",
    "import langchain\n",
    "from langchain import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(max_tokens= 500)\n",
    "loader=UnstructuredURLLoader(urls=[\n",
    "    \"https://arxiv.org/pdf/2201.11903v1.pdf\",\n",
    "    \"https://www.geeksforgeeks.org/machine-learning/\"\n",
    "])\n",
    "data = loader.load()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200\n",
    ")\n",
    "docs = text_splitter.split_documents(data)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorindex_openapi= FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vectorindex_openapi.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorindex_openapi, f)\n",
    "file_path=\"vectorindex_openapi.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        vectorIndex = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQAWithSourcesChain(combine_documents_chain=MapReduceDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template='Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\n{context}\\nQuestion: {question}\\nRelevant text, if any:'), llm=OpenAI(client=<class 'openai.api_resources.completion.Completion'>, max_tokens=500, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')), reduce_documents_chain=ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question', 'summaries'], template='Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\nALWAYS return a \"SOURCES\" part in your answer.\\n\\nQUESTION: Which state/country\\'s law governs the interpretation of the contract?\\n=========\\nContent: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in  relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an  injunction or other relief to protect its Intellectual Property Rights.\\nSource: 28-pl\\nContent: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)  right or remedy.\\n\\n11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation  in force of the remainder of the term (if any) and this Agreement.\\n\\n11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any  kind between the parties.\\n\\n11.9 No Third-Party Beneficiaries.\\nSource: 30-pl\\nContent: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as  defined in Clause 8.5) or that such a violation is reasonably likely to occur,\\nSource: 4-pl\\n=========\\nFINAL ANSWER: This Agreement is governed by English law.\\nSOURCES: 28-pl\\n\\nQUESTION: What did the president say about Michael Jackson?\\n=========\\nContent: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\\nSource: 0-pl\\nContent: And we won’t stop. \\n\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\n\\nLet’s use this moment to reset. Let’s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \\n\\nLet’s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \\n\\nWe can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\n\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\n\\nOfficer Mora was 27 years old. \\n\\nOfficer Rivera was 22. \\n\\nBoth Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \\n\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\\nSource: 24-pl\\nContent: And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \\n\\nTo all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\n\\nAnd I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \\n\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \\n\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \\n\\nThese steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming. \\n\\nBut I want you to know that we are going to be okay.\\nSource: 5-pl\\nContent: More support for patients and families. \\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\n\\nIt’s based on DARPA—the Defense Department project that led to the Internet, GPS, and so much more.  \\n\\nARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more. \\n\\nA unity agenda for the nation. \\n\\nWe can do this. \\n\\nMy fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy. \\n\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\n\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\n\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\n\\nNow is the hour. \\n\\nOur moment of responsibility. \\n\\nOur test of resolve and conscience, of history itself. \\n\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\n\\nWell I know this nation.\\nSource: 34-pl\\n=========\\nFINAL ANSWER: The president did not mention Michael Jackson.\\nSOURCES:\\n\\nQUESTION: {question}\\n=========\\n{summaries}\\n=========\\nFINAL ANSWER:'), llm=OpenAI(client=<class 'openai.api_resources.completion.Completion'>, max_tokens=500, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')), document_prompt=PromptTemplate(input_variables=['page_content', 'source'], template='Content: {page_content}\\nSource: {source}'), document_variable_name='summaries')), document_variable_name='context'), retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x0000028AE7B96590>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vectorIndex.as_retriever())\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the different type of machine learning algorithms ?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input_list\": [\n",
      "    {\n",
      "      \"context\": \"Misc :\\n\\n\\nPattern Recognition | Introduction\\nCalculate Efficiency Of Binary Classifier\\nLogistic Regression v/s Decision Tree Classification\\nR vs Python in Datascience\\nExplanation of Fundamental Functions involved in A3C algorithm\\nDifferential Privacy and Deep Learning\\nArtificial intelligence vs Machine Learning vs Deep Learning\\nIntroduction to Multi-Task Learning(MTL) for Deep Learning\\nTop 10 Algorithms every Machine Learning Engineer should know\\nAzure Virtual Machine for Machine Learning\\n30 minutes to machine learning\\nWhat is AutoML in Machine Learning?\\nConfusion Matrix in Machine Learning\\n\\n\\nPrerequisites to learn machine learning\",\n",
      "      \"question\": \"What are the different type of machine learning algorithms ?\"\n",
      "    },\n",
      "    {\n",
      "      \"context\": \"Prerequisites to learn machine learning\\n\\nKnowledge of Linear equations, graphs of functions, statistics, Linear Algebra, Probability, Calculus etc.\\nAny programming language knowledge like Python, C++, R are recommended.\\nFAQs on Machine Learning Tutorial\\nQ.1 What is Machine learning and how is it different from Deep learning ?\\nAnswer:\\nMachine learning develop programs that can access data and learn from it. Deep learning is the sub domain of the machine learning. Deep learning supports automatic extraction of features from the raw data.\\nQ.2. What are the different type of machine learning algorithms ?\\nAnswer:\",\n",
      "      \"question\": \"What are the different type of machine learning algorithms ?\"\n",
      "    },\n",
      "    {\n",
      "      \"context\": \"Supervised algorithms: These are the algorithms which learn from the labelled data, e.g. images labelled with dog face or not. Algorithm depends on supervised or labelled data. e.g. regression, object detection, segmentation.\\nNon-Supervised algorithms: These are the algorithms which learn from the non labelled data, e.g. bunch of images given to make a similar set of images. e.g. clustering, dimensionality reduction etc.\\nSemi-Supervised algorithms: Algorithms that uses both supervised or non-supervised data. Majority portion of data use for these algorithms are not supervised data. e.g. anamoly detection.\",\n",
      "      \"question\": \"What are the different type of machine learning algorithms ?\"\n",
      "    },\n",
      "    {\n",
      "      \"context\": \"Supervised learning :\\n\\n\\nGetting started with Classification\\nBasic Concept of Classification\\nTypes of Regression Techniques\\nClassification vs Regression\\nML | Types of Learning – Supervised Learning\\nMulticlass classification using scikit-learn\\n\\nGradient Descent :\\nGradient Descent algorithm and its variants\\nStochastic Gradient Descent (SGD)\\nMini-Batch Gradient Descent with Python\\nOptimization techniques for Gradient Descent\\nIntroduction to Momentum-based Gradient Optimizer\",\n",
      "      \"question\": \"What are the different type of machine learning algorithms ?\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 5:llm:OpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\nMisc :\\n\\n\\nPattern Recognition | Introduction\\nCalculate Efficiency Of Binary Classifier\\nLogistic Regression v/s Decision Tree Classification\\nR vs Python in Datascience\\nExplanation of Fundamental Functions involved in A3C algorithm\\nDifferential Privacy and Deep Learning\\nArtificial intelligence vs Machine Learning vs Deep Learning\\nIntroduction to Multi-Task Learning(MTL) for Deep Learning\\nTop 10 Algorithms every Machine Learning Engineer should know\\nAzure Virtual Machine for Machine Learning\\n30 minutes to machine learning\\nWhat is AutoML in Machine Learning?\\nConfusion Matrix in Machine Learning\\n\\n\\nPrerequisites to learn machine learning\\nQuestion: What are the different type of machine learning algorithms ?\\nRelevant text, if any:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 6:llm:OpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\nPrerequisites to learn machine learning\\n\\nKnowledge of Linear equations, graphs of functions, statistics, Linear Algebra, Probability, Calculus etc.\\nAny programming language knowledge like Python, C++, R are recommended.\\nFAQs on Machine Learning Tutorial\\nQ.1 What is Machine learning and how is it different from Deep learning ?\\nAnswer:\\nMachine learning develop programs that can access data and learn from it. Deep learning is the sub domain of the machine learning. Deep learning supports automatic extraction of features from the raw data.\\nQ.2. What are the different type of machine learning algorithms ?\\nAnswer:\\nQuestion: What are the different type of machine learning algorithms ?\\nRelevant text, if any:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 7:llm:OpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\nSupervised algorithms: These are the algorithms which learn from the labelled data, e.g. images labelled with dog face or not. Algorithm depends on supervised or labelled data. e.g. regression, object detection, segmentation.\\nNon-Supervised algorithms: These are the algorithms which learn from the non labelled data, e.g. bunch of images given to make a similar set of images. e.g. clustering, dimensionality reduction etc.\\nSemi-Supervised algorithms: Algorithms that uses both supervised or non-supervised data. Majority portion of data use for these algorithms are not supervised data. e.g. anamoly detection.\\nQuestion: What are the different type of machine learning algorithms ?\\nRelevant text, if any:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 8:llm:OpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\nSupervised learning :\\n\\n\\nGetting started with Classification\\nBasic Concept of Classification\\nTypes of Regression Techniques\\nClassification vs Regression\\nML | Types of Learning – Supervised Learning\\nMulticlass classification using scikit-learn\\n\\nGradient Descent :\\nGradient Descent algorithm and its variants\\nStochastic Gradient Descent (SGD)\\nMini-Batch Gradient Descent with Python\\nOptimization techniques for Gradient Descent\\nIntroduction to Momentum-based Gradient Optimizer\\nQuestion: What are the different type of machine learning algorithms ?\\nRelevant text, if any:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 5:llm:OpenAI] [2.26s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" Top 10 Algorithms every Machine Learning Engineer should know\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"total_tokens\": 863,\n",
      "      \"completion_tokens\": 140,\n",
      "      \"prompt_tokens\": 723\n",
      "    },\n",
      "    \"model_name\": \"text-davinci-003\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 6:llm:OpenAI] [2.25s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\nKnowledge of Linear equations, graphs of functions, statistics, Linear Algebra, Probability, Calculus etc. Any programming language knowledge like Python, C++, R are recommended.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {},\n",
      "    \"model_name\": \"text-davinci-003\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 7:llm:OpenAI] [2.26s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" Supervised algorithms: These are the algorithms which learn from the labelled data, e.g. images labelled with dog face or not. Non-Supervised algorithms: These are the algorithms which learn from the non labelled data, e.g. bunch of images given to make a similar set of images. Semi-Supervised algorithms: Algorithms that uses both supervised or non-supervised data.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {},\n",
      "    \"model_name\": \"text-davinci-003\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 8:llm:OpenAI] [2.26s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"  ML | Types of Learning – Supervised Learning\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {},\n",
      "    \"model_name\": \"text-davinci-003\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain] [2.26s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"text\": \" Top 10 Algorithms every Machine Learning Engineer should know\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"\\nKnowledge of Linear equations, graphs of functions, statistics, Linear Algebra, Probability, Calculus etc. Any programming language knowledge like Python, C++, R are recommended.\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \" Supervised algorithms: These are the algorithms which learn from the labelled data, e.g. images labelled with dog face or not. Non-Supervised algorithms: These are the algorithms which learn from the non labelled data, e.g. bunch of images given to make a similar set of images. Semi-Supervised algorithms: Algorithms that uses both supervised or non-supervised data.\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"  ML | Types of Learning – Supervised Learning\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 9:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the different type of machine learning algorithms ?\",\n",
      "  \"summaries\": \"Content:  Top 10 Algorithms every Machine Learning Engineer should know\\nSource: https://www.geeksforgeeks.org/machine-learning/\\n\\nContent: \\nKnowledge of Linear equations, graphs of functions, statistics, Linear Algebra, Probability, Calculus etc. Any programming language knowledge like Python, C++, R are recommended.\\nSource: https://www.geeksforgeeks.org/machine-learning/\\n\\nContent:  Supervised algorithms: These are the algorithms which learn from the labelled data, e.g. images labelled with dog face or not. Non-Supervised algorithms: These are the algorithms which learn from the non labelled data, e.g. bunch of images given to make a similar set of images. Semi-Supervised algorithms: Algorithms that uses both supervised or non-supervised data.\\nSource: https://www.geeksforgeeks.org/machine-learning/\\n\\nContent:   ML | Types of Learning – Supervised Learning\\nSource: https://www.geeksforgeeks.org/machine-learning/\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 9:chain:LLMChain > 10:llm:OpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Given the following extracted parts of a long document and a question, create a final answer with references (\\\"SOURCES\\\"). \\nIf you don't know the answer, just say that you don't know. Don't try to make up an answer.\\nALWAYS return a \\\"SOURCES\\\" part in your answer.\\n\\nQUESTION: Which state/country's law governs the interpretation of the contract?\\n=========\\nContent: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in  relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an  injunction or other relief to protect its Intellectual Property Rights.\\nSource: 28-pl\\nContent: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)  right or remedy.\\n\\n11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation  in force of the remainder of the term (if any) and this Agreement.\\n\\n11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any  kind between the parties.\\n\\n11.9 No Third-Party Beneficiaries.\\nSource: 30-pl\\nContent: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as  defined in Clause 8.5) or that such a violation is reasonably likely to occur,\\nSource: 4-pl\\n=========\\nFINAL ANSWER: This Agreement is governed by English law.\\nSOURCES: 28-pl\\n\\nQUESTION: What did the president say about Michael Jackson?\\n=========\\nContent: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\\nSource: 0-pl\\nContent: And we won’t stop. \\n\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\n\\nLet’s use this moment to reset. Let’s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \\n\\nLet’s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \\n\\nWe can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\n\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\n\\nOfficer Mora was 27 years old. \\n\\nOfficer Rivera was 22. \\n\\nBoth Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \\n\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\\nSource: 24-pl\\nContent: And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \\n\\nTo all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\n\\nAnd I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \\n\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \\n\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \\n\\nThese steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming. \\n\\nBut I want you to know that we are going to be okay.\\nSource: 5-pl\\nContent: More support for patients and families. \\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\n\\nIt’s based on DARPA—the Defense Department project that led to the Internet, GPS, and so much more.  \\n\\nARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more. \\n\\nA unity agenda for the nation. \\n\\nWe can do this. \\n\\nMy fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy. \\n\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\n\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\n\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\n\\nNow is the hour. \\n\\nOur moment of responsibility. \\n\\nOur test of resolve and conscience, of history itself. \\n\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\n\\nWell I know this nation.\\nSource: 34-pl\\n=========\\nFINAL ANSWER: The president did not mention Michael Jackson.\\nSOURCES:\\n\\nQUESTION: What are the different type of machine learning algorithms ?\\n=========\\nContent:  Top 10 Algorithms every Machine Learning Engineer should know\\nSource: https://www.geeksforgeeks.org/machine-learning/\\n\\nContent: \\nKnowledge of Linear equations, graphs of functions, statistics, Linear Algebra, Probability, Calculus etc. Any programming language knowledge like Python, C++, R are recommended.\\nSource: https://www.geeksforgeeks.org/machine-learning/\\n\\nContent:  Supervised algorithms: These are the algorithms which learn from the labelled data, e.g. images labelled with dog face or not. Non-Supervised algorithms: These are the algorithms which learn from the non labelled data, e.g. bunch of images given to make a similar set of images. Semi-Supervised algorithms: Algorithms that uses both supervised or non-supervised data.\\nSource: https://www.geeksforgeeks.org/machine-learning/\\n\\nContent:   ML | Types of Learning – Supervised Learning\\nSource: https://www.geeksforgeeks.org/machine-learning/\\n=========\\nFINAL ANSWER:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 9:chain:LLMChain > 10:llm:OpenAI] [1.71s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" The different types of machine learning algorithms are supervised, non-supervised, and semi-supervised.\\nSOURCES: https://www.geeksforgeeks.org/machine-learning/\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"total_tokens\": 1783,\n",
      "      \"completion_tokens\": 42,\n",
      "      \"prompt_tokens\": 1741\n",
      "    },\n",
      "    \"model_name\": \"text-davinci-003\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 9:chain:LLMChain] [1.71s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \" The different types of machine learning algorithms are supervised, non-supervised, and semi-supervised.\\nSOURCES: https://www.geeksforgeeks.org/machine-learning/\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain] [9.62s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \" The different types of machine learning algorithms are supervised, non-supervised, and semi-supervised.\\nSOURCES: https://www.geeksforgeeks.org/machine-learning/\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain] [10.12s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"answer\": \" The different types of machine learning algorithms are supervised, non-supervised, and semi-supervised.\\n\",\n",
      "  \"sources\": \"https://www.geeksforgeeks.org/machine-learning/\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': ' The different types of machine learning algorithms are supervised, non-supervised, and semi-supervised.\\n',\n",
       " 'sources': 'https://www.geeksforgeeks.org/machine-learning/'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What are the different type of machine learning algorithms ?\"\n",
    "\n",
    "\n",
    "langchain.debug=True\n",
    "\n",
    "chain({\"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"give me the entire summary for Chain of Thought Prompting Elicits Reasoning in Large Language Models in a easy and simple way\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input_list\": [\n",
      "    {\n",
      "      \"context\": \"Chain of Thought Prompting Elicits Reasoning in Large Language Models\\n\\nMiao, S. Y., Liang, C. C., and Su, K. Y. A diverse cor- pus for evaluating and developing English math word In Proceedings of the 58th Annual problem solvers. Meeting of the Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.92. URL https: //aclanthology.org/2020.acl-main.92.\\n\\nNarang, S., Raffel, C., Lee, K., Roberts, A., Fiedel, N., and Malkan, K. WT5?! Training text-to-text arXiv preprint models to explain their predictions. arXiv:2004.14546, 2020. URL https://arxiv. org/abs/2004.14546.\\n\\nNye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., et al. Show your work: Scratch- pads for intermediate computation with language mod- arXiv preprint arXiv:2112.00114, 2021. URL els. http://arxiv.org/abs/2112.00114.\\n\\nD19-1251. URL https://aclanthology.org/ D19-1251.\",\n",
      "      \"question\": \"give me the entire summary for Chain of Thought Prompting Elicits Reasoning in Large Language Models in a easy and simple way\"\n",
      "    },\n",
      "    {\n",
      "      \"context\": \"Chain of Thought Prompting Elicits Reasoning in Large Language Models\\n\\nsolving an entire multi-hop problem in a single forward pass. Chain of thought prompting is simple and improves performance across various reasoning tasks where standard few-shot prompting is insufﬁcient. Crucially, our experi- ments suggest that successful chain of thought prompting is an emergent property of model scale—that is, the beneﬁts of chain of thought prompting only materialize at sufﬁcient model scale (around 100B parameters).\\n\\n(Section 4), and commonsense reasoning (Section 5), we show that chain of thought prompting dramatically improves performance on several datasets where standard few-shot prompting is insufﬁcient.\\n\\n3. Arithmetic Reasoning\\n\\n2. Chain of Thought\",\n",
      "      \"question\": \"give me the entire summary for Chain of Thought Prompting Elicits Reasoning in Large Language Models in a easy and simple way\"\n",
      "    },\n",
      "    {\n",
      "      \"context\": \"Chain of Thought Prompting Elicits Reasoning in Large Language Models\\n\\nThe emergence of chain of thought reasoning as a conse- quence of model scale has been a prevailing theme of these experiments. For six reasoning tasks where standard prompt- ing has a ﬂat scaling curve, chain of thought prompting leads to dramatically increasing scaling curves for sufﬁciently large language models. Chain of thought prompting appears to expand the set of tasks that large language models can per- form successfully—in other words, our work underscores that standard prompting only provides a lower bound on the capabilities of large language models in principle. This ob- servation likely raises more questions than it answers—for instance, how much more can we expect reasoning ability to improve with a further increase in model scale? What other prompting methods might expand the range of tasks that language models can solve?\",\n",
      "      \"question\": \"give me the entire summary for Chain of Thought Prompting Elicits Reasoning in Large Language Models in a easy and simple way\"\n",
      "    },\n",
      "    {\n",
      "      \"context\": \"8. Conclusions\\n\\nWe have explored chain of thought prompting as a sim- ple and broadly applicable method for enhancing reason- ing in language models. Through experiments on arith- metic, symbolic, and commonsense reasoning, we ﬁnd that chain of thought processing is an emergent property of model scale that allows sufﬁciently large language models to perform reasoning tasks that otherwise have ﬂat scaling curves. Broadening the range of reasoning tasks that lan- guage models can perform will hopefully inspire further work on language-based approaches to reasoning.\\n\\nChain of Thought Prompting Elicits Reasoning in Large Language Models\\n\\nReproducibility Statement\",\n",
      "      \"question\": \"give me the entire summary for Chain of Thought Prompting Elicits Reasoning in Large Language Models in a easy and simple way\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 5:llm:OpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\nChain of Thought Prompting Elicits Reasoning in Large Language Models\\n\\nMiao, S. Y., Liang, C. C., and Su, K. Y. A diverse cor- pus for evaluating and developing English math word In Proceedings of the 58th Annual problem solvers. Meeting of the Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.92. URL https: //aclanthology.org/2020.acl-main.92.\\n\\nNarang, S., Raffel, C., Lee, K., Roberts, A., Fiedel, N., and Malkan, K. WT5?! Training text-to-text arXiv preprint models to explain their predictions. arXiv:2004.14546, 2020. URL https://arxiv. org/abs/2004.14546.\\n\\nNye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., et al. Show your work: Scratch- pads for intermediate computation with language mod- arXiv preprint arXiv:2112.00114, 2021. URL els. http://arxiv.org/abs/2112.00114.\\n\\nD19-1251. URL https://aclanthology.org/ D19-1251.\\nQuestion: give me the entire summary for Chain of Thought Prompting Elicits Reasoning in Large Language Models in a easy and simple way\\nRelevant text, if any:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 6:llm:OpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\nChain of Thought Prompting Elicits Reasoning in Large Language Models\\n\\nsolving an entire multi-hop problem in a single forward pass. Chain of thought prompting is simple and improves performance across various reasoning tasks where standard few-shot prompting is insufﬁcient. Crucially, our experi- ments suggest that successful chain of thought prompting is an emergent property of model scale—that is, the beneﬁts of chain of thought prompting only materialize at sufﬁcient model scale (around 100B parameters).\\n\\n(Section 4), and commonsense reasoning (Section 5), we show that chain of thought prompting dramatically improves performance on several datasets where standard few-shot prompting is insufﬁcient.\\n\\n3. Arithmetic Reasoning\\n\\n2. Chain of Thought\\nQuestion: give me the entire summary for Chain of Thought Prompting Elicits Reasoning in Large Language Models in a easy and simple way\\nRelevant text, if any:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 7:llm:OpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\nChain of Thought Prompting Elicits Reasoning in Large Language Models\\n\\nThe emergence of chain of thought reasoning as a conse- quence of model scale has been a prevailing theme of these experiments. For six reasoning tasks where standard prompt- ing has a ﬂat scaling curve, chain of thought prompting leads to dramatically increasing scaling curves for sufﬁciently large language models. Chain of thought prompting appears to expand the set of tasks that large language models can per- form successfully—in other words, our work underscores that standard prompting only provides a lower bound on the capabilities of large language models in principle. This ob- servation likely raises more questions than it answers—for instance, how much more can we expect reasoning ability to improve with a further increase in model scale? What other prompting methods might expand the range of tasks that language models can solve?\\nQuestion: give me the entire summary for Chain of Thought Prompting Elicits Reasoning in Large Language Models in a easy and simple way\\nRelevant text, if any:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 8:llm:OpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\n8. Conclusions\\n\\nWe have explored chain of thought prompting as a sim- ple and broadly applicable method for enhancing reason- ing in language models. Through experiments on arith- metic, symbolic, and commonsense reasoning, we ﬁnd that chain of thought processing is an emergent property of model scale that allows sufﬁciently large language models to perform reasoning tasks that otherwise have ﬂat scaling curves. Broadening the range of reasoning tasks that lan- guage models can perform will hopefully inspire further work on language-based approaches to reasoning.\\n\\nChain of Thought Prompting Elicits Reasoning in Large Language Models\\n\\nReproducibility Statement\\nQuestion: give me the entire summary for Chain of Thought Prompting Elicits Reasoning in Large Language Models in a easy and simple way\\nRelevant text, if any:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 5:llm:OpenAI] [2.48s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" None\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"total_tokens\": 1324,\n",
      "      \"completion_tokens\": 229,\n",
      "      \"prompt_tokens\": 1095\n",
      "    },\n",
      "    \"model_name\": \"text-davinci-003\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 6:llm:OpenAI] [2.48s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" Chain of thought prompting is simple and improves performance across various reasoning tasks where standard few-shot prompting is insufficient. Crucially, our experiments suggest that successful chain of thought prompting is an emergent property of model scale—that is, the benefits of chain of thought prompting only materialize at sufficient model scale (around 100B parameters). We show that chain of thought prompting dramatically improves performance on several datasets where standard few-shot prompting is insufficient.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {},\n",
      "    \"model_name\": \"text-davinci-003\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 7:llm:OpenAI] [2.48s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" \\nThis work demonstrates that chain of thought prompting leads to improved reasoning capabilities in large language models compared to standard prompting. This suggests that language models may have even greater potential for reasoning tasks with further increases in model scale, raising questions about other prompting methods that can expand the range of tasks language models can solve.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {},\n",
      "    \"model_name\": \"text-davinci-003\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 8:llm:OpenAI] [2.48s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" We have explored chain of thought prompting as a simple and broadly applicable method for enhancing reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense reasoning, we find that chain of thought processing is an emergent property of model scale that allows sufficiently large language models to perform reasoning tasks that otherwise have flat scaling curves, broadening the range of reasoning tasks that language models can perform.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {},\n",
      "    \"model_name\": \"text-davinci-003\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain] [2.48s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"text\": \" None\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \" Chain of thought prompting is simple and improves performance across various reasoning tasks where standard few-shot prompting is insufficient. Crucially, our experiments suggest that successful chain of thought prompting is an emergent property of model scale—that is, the benefits of chain of thought prompting only materialize at sufficient model scale (around 100B parameters). We show that chain of thought prompting dramatically improves performance on several datasets where standard few-shot prompting is insufficient.\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \" \\nThis work demonstrates that chain of thought prompting leads to improved reasoning capabilities in large language models compared to standard prompting. This suggests that language models may have even greater potential for reasoning tasks with further increases in model scale, raising questions about other prompting methods that can expand the range of tasks language models can solve.\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \" We have explored chain of thought prompting as a simple and broadly applicable method for enhancing reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense reasoning, we find that chain of thought processing is an emergent property of model scale that allows sufficiently large language models to perform reasoning tasks that otherwise have flat scaling curves, broadening the range of reasoning tasks that language models can perform.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 9:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"give me the entire summary for Chain of Thought Prompting Elicits Reasoning in Large Language Models in a easy and simple way\",\n",
      "  \"summaries\": \"Content:  None\\nSource: https://arxiv.org/pdf/2201.11903v1.pdf\\n\\nContent:  Chain of thought prompting is simple and improves performance across various reasoning tasks where standard few-shot prompting is insufficient. Crucially, our experiments suggest that successful chain of thought prompting is an emergent property of model scale—that is, the benefits of chain of thought prompting only materialize at sufficient model scale (around 100B parameters). We show that chain of thought prompting dramatically improves performance on several datasets where standard few-shot prompting is insufficient.\\nSource: https://arxiv.org/pdf/2201.11903v1.pdf\\n\\nContent:  \\nThis work demonstrates that chain of thought prompting leads to improved reasoning capabilities in large language models compared to standard prompting. This suggests that language models may have even greater potential for reasoning tasks with further increases in model scale, raising questions about other prompting methods that can expand the range of tasks language models can solve.\\nSource: https://arxiv.org/pdf/2201.11903v1.pdf\\n\\nContent:  We have explored chain of thought prompting as a simple and broadly applicable method for enhancing reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense reasoning, we find that chain of thought processing is an emergent property of model scale that allows sufficiently large language models to perform reasoning tasks that otherwise have flat scaling curves, broadening the range of reasoning tasks that language models can perform.\\nSource: https://arxiv.org/pdf/2201.11903v1.pdf\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 9:chain:LLMChain > 10:llm:OpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Given the following extracted parts of a long document and a question, create a final answer with references (\\\"SOURCES\\\"). \\nIf you don't know the answer, just say that you don't know. Don't try to make up an answer.\\nALWAYS return a \\\"SOURCES\\\" part in your answer.\\n\\nQUESTION: Which state/country's law governs the interpretation of the contract?\\n=========\\nContent: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in  relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an  injunction or other relief to protect its Intellectual Property Rights.\\nSource: 28-pl\\nContent: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)  right or remedy.\\n\\n11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation  in force of the remainder of the term (if any) and this Agreement.\\n\\n11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any  kind between the parties.\\n\\n11.9 No Third-Party Beneficiaries.\\nSource: 30-pl\\nContent: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as  defined in Clause 8.5) or that such a violation is reasonably likely to occur,\\nSource: 4-pl\\n=========\\nFINAL ANSWER: This Agreement is governed by English law.\\nSOURCES: 28-pl\\n\\nQUESTION: What did the president say about Michael Jackson?\\n=========\\nContent: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\\nSource: 0-pl\\nContent: And we won’t stop. \\n\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\n\\nLet’s use this moment to reset. Let’s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \\n\\nLet’s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \\n\\nWe can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\n\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\n\\nOfficer Mora was 27 years old. \\n\\nOfficer Rivera was 22. \\n\\nBoth Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \\n\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\\nSource: 24-pl\\nContent: And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \\n\\nTo all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\n\\nAnd I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \\n\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \\n\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \\n\\nThese steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming. \\n\\nBut I want you to know that we are going to be okay.\\nSource: 5-pl\\nContent: More support for patients and families. \\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\n\\nIt’s based on DARPA—the Defense Department project that led to the Internet, GPS, and so much more.  \\n\\nARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more. \\n\\nA unity agenda for the nation. \\n\\nWe can do this. \\n\\nMy fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy. \\n\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\n\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\n\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\n\\nNow is the hour. \\n\\nOur moment of responsibility. \\n\\nOur test of resolve and conscience, of history itself. \\n\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\n\\nWell I know this nation.\\nSource: 34-pl\\n=========\\nFINAL ANSWER: The president did not mention Michael Jackson.\\nSOURCES:\\n\\nQUESTION: give me the entire summary for Chain of Thought Prompting Elicits Reasoning in Large Language Models in a easy and simple way\\n=========\\nContent:  None\\nSource: https://arxiv.org/pdf/2201.11903v1.pdf\\n\\nContent:  Chain of thought prompting is simple and improves performance across various reasoning tasks where standard few-shot prompting is insufficient. Crucially, our experiments suggest that successful chain of thought prompting is an emergent property of model scale—that is, the benefits of chain of thought prompting only materialize at sufficient model scale (around 100B parameters). We show that chain of thought prompting dramatically improves performance on several datasets where standard few-shot prompting is insufficient.\\nSource: https://arxiv.org/pdf/2201.11903v1.pdf\\n\\nContent:  \\nThis work demonstrates that chain of thought prompting leads to improved reasoning capabilities in large language models compared to standard prompting. This suggests that language models may have even greater potential for reasoning tasks with further increases in model scale, raising questions about other prompting methods that can expand the range of tasks language models can solve.\\nSource: https://arxiv.org/pdf/2201.11903v1.pdf\\n\\nContent:  We have explored chain of thought prompting as a simple and broadly applicable method for enhancing reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense reasoning, we find that chain of thought processing is an emergent property of model scale that allows sufficiently large language models to perform reasoning tasks that otherwise have flat scaling curves, broadening the range of reasoning tasks that language models can perform.\\nSource: https://arxiv.org/pdf/2201.11903v1.pdf\\n=========\\nFINAL ANSWER:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 9:chain:LLMChain > 10:llm:OpenAI] [2.70s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" This work demonstrates that chain of thought prompting leads to improved reasoning capabilities in large language models compared to standard prompting. Experiments on arithmetic, symbolic, and commonsense reasoning showed that chain of thought processing is an emergent property of model scale which allows sufficiently large language models to perform reasoning tasks that have flat scaling curves, broadening the range of reasoning tasks that language models can perform.\\nSOURCES: https://arxiv.org/pdf/2201.11903v1.pdf\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"total_tokens\": 1963,\n",
      "      \"completion_tokens\": 101,\n",
      "      \"prompt_tokens\": 1862\n",
      "    },\n",
      "    \"model_name\": \"text-davinci-003\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 9:chain:LLMChain] [2.70s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \" This work demonstrates that chain of thought prompting leads to improved reasoning capabilities in large language models compared to standard prompting. Experiments on arithmetic, symbolic, and commonsense reasoning showed that chain of thought processing is an emergent property of model scale which allows sufficiently large language models to perform reasoning tasks that have flat scaling curves, broadening the range of reasoning tasks that language models can perform.\\nSOURCES: https://arxiv.org/pdf/2201.11903v1.pdf\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain] [5.19s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \" This work demonstrates that chain of thought prompting leads to improved reasoning capabilities in large language models compared to standard prompting. Experiments on arithmetic, symbolic, and commonsense reasoning showed that chain of thought processing is an emergent property of model scale which allows sufficiently large language models to perform reasoning tasks that have flat scaling curves, broadening the range of reasoning tasks that language models can perform.\\nSOURCES: https://arxiv.org/pdf/2201.11903v1.pdf\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain] [5.72s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"answer\": \" This work demonstrates that chain of thought prompting leads to improved reasoning capabilities in large language models compared to standard prompting. Experiments on arithmetic, symbolic, and commonsense reasoning showed that chain of thought processing is an emergent property of model scale which allows sufficiently large language models to perform reasoning tasks that have flat scaling curves, broadening the range of reasoning tasks that language models can perform.\\n\",\n",
      "  \"sources\": \"https://arxiv.org/pdf/2201.11903v1.pdf\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': ' This work demonstrates that chain of thought prompting leads to improved reasoning capabilities in large language models compared to standard prompting. Experiments on arithmetic, symbolic, and commonsense reasoning showed that chain of thought processing is an emergent property of model scale which allows sufficiently large language models to perform reasoning tasks that have flat scaling curves, broadening the range of reasoning tasks that language models can perform.\\n',\n",
       " 'sources': 'https://arxiv.org/pdf/2201.11903v1.pdf'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"give me the entire summary for Chain of Thought Prompting Elicits Reasoning in Large Language Models in a easy and simple way\"\n",
    "\n",
    "\n",
    "langchain.debug=True\n",
    "\n",
    "chain({\"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"important questions on ML along with its answer\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input_list\": [\n",
      "    {\n",
      "      \"context\": \"Misc :\\n\\n\\nPattern Recognition | Introduction\\nCalculate Efficiency Of Binary Classifier\\nLogistic Regression v/s Decision Tree Classification\\nR vs Python in Datascience\\nExplanation of Fundamental Functions involved in A3C algorithm\\nDifferential Privacy and Deep Learning\\nArtificial intelligence vs Machine Learning vs Deep Learning\\nIntroduction to Multi-Task Learning(MTL) for Deep Learning\\nTop 10 Algorithms every Machine Learning Engineer should know\\nAzure Virtual Machine for Machine Learning\\n30 minutes to machine learning\\nWhat is AutoML in Machine Learning?\\nConfusion Matrix in Machine Learning\\n\\n\\nPrerequisites to learn machine learning\",\n",
      "      \"question\": \"important questions on ML along with its answer\"\n",
      "    },\n",
      "    {\n",
      "      \"context\": \"Explore Our Geeks CommunityWrite an Interview ExperienceShare Your Campus ExperienceMachine Learning TutorialGetting Started with Machine LearningAn introduction to Machine LearningGetting started with Machine LearningWhat is Machine Learning?Best Python libraries for Machine LearningDifference Between Machine Learning and Artificial IntelligenceGeneral steps to follow in a Machine Learning ProblemMachine Learning MathematicsData PreprocessingML | Introduction to Data in Machine LearningML | Understanding Data ProcessingPython | Create Test DataSets using SklearnGenerate Test Datasets for Machine learningML | Overview of Data CleaningOne Hot Encoding in Machine LearningML | Dummy variable trap in Regression ModelsWhat is Exploratory Data Analysis ?ML | Feature Scaling - Part 1Feature Engineering: Scaling, Normalization, and StandardizationLabel Encoding in PythonML | Handling Imbalanced Data with SMOTE and Near Miss Algorithm in PythonClassification & RegressionOrdinary Least\",\n",
      "      \"question\": \"important questions on ML along with its answer\"\n",
      "    },\n",
      "    {\n",
      "      \"context\": \"Explore Our Geeks CommunityWrite an Interview ExperienceShare Your Campus ExperienceMachine Learning TutorialGetting Started with Machine LearningAn introduction to Machine LearningGetting started with Machine LearningWhat is Machine Learning?Best Python libraries for Machine LearningDifference Between Machine Learning and Artificial IntelligenceGeneral steps to follow in a Machine Learning ProblemMachine Learning MathematicsData PreprocessingML | Introduction to Data in Machine LearningML | Understanding Data ProcessingPython | Create Test DataSets using SklearnGenerate Test Datasets for Machine learningML | Overview of Data CleaningOne Hot Encoding in Machine LearningML | Dummy variable trap in Regression ModelsWhat is Exploratory Data Analysis ?ML | Feature Scaling - Part 1Feature Engineering: Scaling, Normalization, and StandardizationLabel Encoding in PythonML | Handling Imbalanced Data with SMOTE and Near Miss Algorithm in PythonClassification & RegressionOrdinary Least\",\n",
      "      \"question\": \"important questions on ML along with its answer\"\n",
      "    },\n",
      "    {\n",
      "      \"context\": \"Q.3. Why we use machine learning ?\\nAnswer:\\nMachine learning is used to make decisions based on data. By modelling the algorithms on the bases of historical data, Algorithms find the patterns and relationships that are difficult for humans to detect. These patterns are now further use for the future references to predict solution of unseen problems.\\nQ.4. What is the difference between Artificial Intelligence and Machine learning ?\\nAnswer:\\n\\n\\n\\nArtificial Intelligence\\nMachine Learning\\n\\n\\nDevelop an intelligent system  that perform variety of complex jobs.\\nConstruct machines that can only accomplish the jobs for which they have trained. \\n\\n\\nIt works as a program that does smart work.\\nThe tasks systems machine takes data and learns from data.\\n\\n\\nAI has broad variety of applications.\\nML allows systems to learn new things from data.\\n\\n\\nAI leads wisdom.\\nML leads to knowledge.\",\n",
      "      \"question\": \"important questions on ML along with its answer\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 5:llm:OpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\nMisc :\\n\\n\\nPattern Recognition | Introduction\\nCalculate Efficiency Of Binary Classifier\\nLogistic Regression v/s Decision Tree Classification\\nR vs Python in Datascience\\nExplanation of Fundamental Functions involved in A3C algorithm\\nDifferential Privacy and Deep Learning\\nArtificial intelligence vs Machine Learning vs Deep Learning\\nIntroduction to Multi-Task Learning(MTL) for Deep Learning\\nTop 10 Algorithms every Machine Learning Engineer should know\\nAzure Virtual Machine for Machine Learning\\n30 minutes to machine learning\\nWhat is AutoML in Machine Learning?\\nConfusion Matrix in Machine Learning\\n\\n\\nPrerequisites to learn machine learning\\nQuestion: important questions on ML along with its answer\\nRelevant text, if any:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 6:llm:OpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\nExplore Our Geeks CommunityWrite an Interview ExperienceShare Your Campus ExperienceMachine Learning TutorialGetting Started with Machine LearningAn introduction to Machine LearningGetting started with Machine LearningWhat is Machine Learning?Best Python libraries for Machine LearningDifference Between Machine Learning and Artificial IntelligenceGeneral steps to follow in a Machine Learning ProblemMachine Learning MathematicsData PreprocessingML | Introduction to Data in Machine LearningML | Understanding Data ProcessingPython | Create Test DataSets using SklearnGenerate Test Datasets for Machine learningML | Overview of Data CleaningOne Hot Encoding in Machine LearningML | Dummy variable trap in Regression ModelsWhat is Exploratory Data Analysis ?ML | Feature Scaling - Part 1Feature Engineering: Scaling, Normalization, and StandardizationLabel Encoding in PythonML | Handling Imbalanced Data with SMOTE and Near Miss Algorithm in PythonClassification & RegressionOrdinary Least\\nQuestion: important questions on ML along with its answer\\nRelevant text, if any:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 7:llm:OpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\nExplore Our Geeks CommunityWrite an Interview ExperienceShare Your Campus ExperienceMachine Learning TutorialGetting Started with Machine LearningAn introduction to Machine LearningGetting started with Machine LearningWhat is Machine Learning?Best Python libraries for Machine LearningDifference Between Machine Learning and Artificial IntelligenceGeneral steps to follow in a Machine Learning ProblemMachine Learning MathematicsData PreprocessingML | Introduction to Data in Machine LearningML | Understanding Data ProcessingPython | Create Test DataSets using SklearnGenerate Test Datasets for Machine learningML | Overview of Data CleaningOne Hot Encoding in Machine LearningML | Dummy variable trap in Regression ModelsWhat is Exploratory Data Analysis ?ML | Feature Scaling - Part 1Feature Engineering: Scaling, Normalization, and StandardizationLabel Encoding in PythonML | Handling Imbalanced Data with SMOTE and Near Miss Algorithm in PythonClassification & RegressionOrdinary Least\\nQuestion: important questions on ML along with its answer\\nRelevant text, if any:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 8:llm:OpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\nQ.3. Why we use machine learning ?\\nAnswer:\\nMachine learning is used to make decisions based on data. By modelling the algorithms on the bases of historical data, Algorithms find the patterns and relationships that are difficult for humans to detect. These patterns are now further use for the future references to predict solution of unseen problems.\\nQ.4. What is the difference between Artificial Intelligence and Machine learning ?\\nAnswer:\\n\\n\\n\\nArtificial Intelligence\\nMachine Learning\\n\\n\\nDevelop an intelligent system  that perform variety of complex jobs.\\nConstruct machines that can only accomplish the jobs for which they have trained. \\n\\n\\nIt works as a program that does smart work.\\nThe tasks systems machine takes data and learns from data.\\n\\n\\nAI has broad variety of applications.\\nML allows systems to learn new things from data.\\n\\n\\nAI leads wisdom.\\nML leads to knowledge.\\nQuestion: important questions on ML along with its answer\\nRelevant text, if any:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 5:llm:OpenAI] [4.19s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" 30 minutes to machine learning, Top 10 Algorithms every Machine Learning Engineer should know, What is AutoML in Machine Learning?\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"total_tokens\": 1320,\n",
      "      \"completion_tokens\": 439,\n",
      "      \"prompt_tokens\": 881\n",
      "    },\n",
      "    \"model_name\": \"text-davinci-003\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 6:llm:OpenAI] [4.19s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\nWhat is Machine Learning?\\nBest Python libraries for Machine Learning\\nDifference Between Machine Learning and Artificial Intelligence\\nGeneral steps to follow in a Machine Learning Problem\\nMachine Learning Mathematics\\nData Preprocessing\\nML | Introduction to Data in Machine Learning\\nML | Understanding Data Processing\\nPython | Create Test DataSets using Sklearn\\nGenerate Test Datasets for Machine learning\\nML | Overview of Data Cleaning\\nOne Hot Encoding in Machine Learning\\nML | Dummy variable trap in Regression Models\\nWhat is Exploratory Data Analysis ?\\nML | Feature Scaling - Part 1\\nFeature Engineering: Scaling, Normalization, and Standardization\\nLabel Encoding in Python\\nML | Handling Imbalanced Data with SMOTE and Near Miss Algorithm in Python\\nClassification & Regression\\nOrdinary Least\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {},\n",
      "    \"model_name\": \"text-davinci-003\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 7:llm:OpenAI] [4.19s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" \\nWhat is Machine Learning? \\nBest Python libraries for Machine Learning \\nDifference Between Machine Learning and Artificial Intelligence \\nGeneral steps to follow in a Machine Learning Problem \\nMachine Learning Mathematics \\nData Preprocessing \\nML | Introduction to Data in Machine Learning \\nML | Understanding Data Processing \\nPython | Create Test DataSets using Sklearn \\nGenerate Test Datasets for Machine learning \\nML | Overview of Data Cleaning \\nOne Hot Encoding in Machine Learning \\nML | Dummy variable trap in Regression Models \\nWhat is Exploratory Data Analysis ? \\nML | Feature Scaling - Part 1 \\nFeature Engineering: Scaling, Normalization, and Standardization \\nLabel Encoding in Python \\nML | Handling Imbalanced Data with SMOTE and Near Miss Algorithm in Python \\nClassification & Regression \\nOrdinary Least Squares\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {},\n",
      "    \"model_name\": \"text-davinci-003\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 8:llm:OpenAI] [4.19s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" Machine learning is used to make decisions based on data. By modelling the algorithms on the bases of historical data, Algorithms find the patterns and relationships that are difficult for humans to detect. These patterns are now further use for the future references to predict solution of unseen problems.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {},\n",
      "    \"model_name\": \"text-davinci-003\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain] [4.19s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"text\": \" 30 minutes to machine learning, Top 10 Algorithms every Machine Learning Engineer should know, What is AutoML in Machine Learning?\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"\\nWhat is Machine Learning?\\nBest Python libraries for Machine Learning\\nDifference Between Machine Learning and Artificial Intelligence\\nGeneral steps to follow in a Machine Learning Problem\\nMachine Learning Mathematics\\nData Preprocessing\\nML | Introduction to Data in Machine Learning\\nML | Understanding Data Processing\\nPython | Create Test DataSets using Sklearn\\nGenerate Test Datasets for Machine learning\\nML | Overview of Data Cleaning\\nOne Hot Encoding in Machine Learning\\nML | Dummy variable trap in Regression Models\\nWhat is Exploratory Data Analysis ?\\nML | Feature Scaling - Part 1\\nFeature Engineering: Scaling, Normalization, and Standardization\\nLabel Encoding in Python\\nML | Handling Imbalanced Data with SMOTE and Near Miss Algorithm in Python\\nClassification & Regression\\nOrdinary Least\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \" \\nWhat is Machine Learning? \\nBest Python libraries for Machine Learning \\nDifference Between Machine Learning and Artificial Intelligence \\nGeneral steps to follow in a Machine Learning Problem \\nMachine Learning Mathematics \\nData Preprocessing \\nML | Introduction to Data in Machine Learning \\nML | Understanding Data Processing \\nPython | Create Test DataSets using Sklearn \\nGenerate Test Datasets for Machine learning \\nML | Overview of Data Cleaning \\nOne Hot Encoding in Machine Learning \\nML | Dummy variable trap in Regression Models \\nWhat is Exploratory Data Analysis ? \\nML | Feature Scaling - Part 1 \\nFeature Engineering: Scaling, Normalization, and Standardization \\nLabel Encoding in Python \\nML | Handling Imbalanced Data with SMOTE and Near Miss Algorithm in Python \\nClassification & Regression \\nOrdinary Least Squares\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \" Machine learning is used to make decisions based on data. By modelling the algorithms on the bases of historical data, Algorithms find the patterns and relationships that are difficult for humans to detect. These patterns are now further use for the future references to predict solution of unseen problems.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 9:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"important questions on ML along with its answer\",\n",
      "  \"summaries\": \"Content:  30 minutes to machine learning, Top 10 Algorithms every Machine Learning Engineer should know, What is AutoML in Machine Learning?\\nSource: https://www.geeksforgeeks.org/machine-learning/\\n\\nContent: \\nWhat is Machine Learning?\\nBest Python libraries for Machine Learning\\nDifference Between Machine Learning and Artificial Intelligence\\nGeneral steps to follow in a Machine Learning Problem\\nMachine Learning Mathematics\\nData Preprocessing\\nML | Introduction to Data in Machine Learning\\nML | Understanding Data Processing\\nPython | Create Test DataSets using Sklearn\\nGenerate Test Datasets for Machine learning\\nML | Overview of Data Cleaning\\nOne Hot Encoding in Machine Learning\\nML | Dummy variable trap in Regression Models\\nWhat is Exploratory Data Analysis ?\\nML | Feature Scaling - Part 1\\nFeature Engineering: Scaling, Normalization, and Standardization\\nLabel Encoding in Python\\nML | Handling Imbalanced Data with SMOTE and Near Miss Algorithm in Python\\nClassification & Regression\\nOrdinary Least\\nSource: https://www.geeksforgeeks.org/machine-learning/\\n\\nContent:  \\nWhat is Machine Learning? \\nBest Python libraries for Machine Learning \\nDifference Between Machine Learning and Artificial Intelligence \\nGeneral steps to follow in a Machine Learning Problem \\nMachine Learning Mathematics \\nData Preprocessing \\nML | Introduction to Data in Machine Learning \\nML | Understanding Data Processing \\nPython | Create Test DataSets using Sklearn \\nGenerate Test Datasets for Machine learning \\nML | Overview of Data Cleaning \\nOne Hot Encoding in Machine Learning \\nML | Dummy variable trap in Regression Models \\nWhat is Exploratory Data Analysis ? \\nML | Feature Scaling - Part 1 \\nFeature Engineering: Scaling, Normalization, and Standardization \\nLabel Encoding in Python \\nML | Handling Imbalanced Data with SMOTE and Near Miss Algorithm in Python \\nClassification & Regression \\nOrdinary Least Squares\\nSource: https://www.geeksforgeeks.org/machine-learning/\\n\\nContent:  Machine learning is used to make decisions based on data. By modelling the algorithms on the bases of historical data, Algorithms find the patterns and relationships that are difficult for humans to detect. These patterns are now further use for the future references to predict solution of unseen problems.\\nSource: https://www.geeksforgeeks.org/machine-learning/\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 9:chain:LLMChain > 10:llm:OpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Given the following extracted parts of a long document and a question, create a final answer with references (\\\"SOURCES\\\"). \\nIf you don't know the answer, just say that you don't know. Don't try to make up an answer.\\nALWAYS return a \\\"SOURCES\\\" part in your answer.\\n\\nQUESTION: Which state/country's law governs the interpretation of the contract?\\n=========\\nContent: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in  relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an  injunction or other relief to protect its Intellectual Property Rights.\\nSource: 28-pl\\nContent: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)  right or remedy.\\n\\n11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation  in force of the remainder of the term (if any) and this Agreement.\\n\\n11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any  kind between the parties.\\n\\n11.9 No Third-Party Beneficiaries.\\nSource: 30-pl\\nContent: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as  defined in Clause 8.5) or that such a violation is reasonably likely to occur,\\nSource: 4-pl\\n=========\\nFINAL ANSWER: This Agreement is governed by English law.\\nSOURCES: 28-pl\\n\\nQUESTION: What did the president say about Michael Jackson?\\n=========\\nContent: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\\nSource: 0-pl\\nContent: And we won’t stop. \\n\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\n\\nLet’s use this moment to reset. Let’s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \\n\\nLet’s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \\n\\nWe can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\n\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\n\\nOfficer Mora was 27 years old. \\n\\nOfficer Rivera was 22. \\n\\nBoth Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \\n\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\\nSource: 24-pl\\nContent: And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \\n\\nTo all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\n\\nAnd I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \\n\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \\n\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \\n\\nThese steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming. \\n\\nBut I want you to know that we are going to be okay.\\nSource: 5-pl\\nContent: More support for patients and families. \\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\n\\nIt’s based on DARPA—the Defense Department project that led to the Internet, GPS, and so much more.  \\n\\nARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more. \\n\\nA unity agenda for the nation. \\n\\nWe can do this. \\n\\nMy fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy. \\n\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\n\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\n\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\n\\nNow is the hour. \\n\\nOur moment of responsibility. \\n\\nOur test of resolve and conscience, of history itself. \\n\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\n\\nWell I know this nation.\\nSource: 34-pl\\n=========\\nFINAL ANSWER: The president did not mention Michael Jackson.\\nSOURCES:\\n\\nQUESTION: important questions on ML along with its answer\\n=========\\nContent:  30 minutes to machine learning, Top 10 Algorithms every Machine Learning Engineer should know, What is AutoML in Machine Learning?\\nSource: https://www.geeksforgeeks.org/machine-learning/\\n\\nContent: \\nWhat is Machine Learning?\\nBest Python libraries for Machine Learning\\nDifference Between Machine Learning and Artificial Intelligence\\nGeneral steps to follow in a Machine Learning Problem\\nMachine Learning Mathematics\\nData Preprocessing\\nML | Introduction to Data in Machine Learning\\nML | Understanding Data Processing\\nPython | Create Test DataSets using Sklearn\\nGenerate Test Datasets for Machine learning\\nML | Overview of Data Cleaning\\nOne Hot Encoding in Machine Learning\\nML | Dummy variable trap in Regression Models\\nWhat is Exploratory Data Analysis ?\\nML | Feature Scaling - Part 1\\nFeature Engineering: Scaling, Normalization, and Standardization\\nLabel Encoding in Python\\nML | Handling Imbalanced Data with SMOTE and Near Miss Algorithm in Python\\nClassification & Regression\\nOrdinary Least\\nSource: https://www.geeksforgeeks.org/machine-learning/\\n\\nContent:  \\nWhat is Machine Learning? \\nBest Python libraries for Machine Learning \\nDifference Between Machine Learning and Artificial Intelligence \\nGeneral steps to follow in a Machine Learning Problem \\nMachine Learning Mathematics \\nData Preprocessing \\nML | Introduction to Data in Machine Learning \\nML | Understanding Data Processing \\nPython | Create Test DataSets using Sklearn \\nGenerate Test Datasets for Machine learning \\nML | Overview of Data Cleaning \\nOne Hot Encoding in Machine Learning \\nML | Dummy variable trap in Regression Models \\nWhat is Exploratory Data Analysis ? \\nML | Feature Scaling - Part 1 \\nFeature Engineering: Scaling, Normalization, and Standardization \\nLabel Encoding in Python \\nML | Handling Imbalanced Data with SMOTE and Near Miss Algorithm in Python \\nClassification & Regression \\nOrdinary Least Squares\\nSource: https://www.geeksforgeeks.org/machine-learning/\\n\\nContent:  Machine learning is used to make decisions based on data. By modelling the algorithms on the bases of historical data, Algorithms find the patterns and relationships that are difficult for humans to detect. These patterns are now further use for the future references to predict solution of unseen problems.\\nSource: https://www.geeksforgeeks.org/machine-learning/\\n=========\\nFINAL ANSWER:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 9:chain:LLMChain > 10:llm:OpenAI] [2.56s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" Machine learning is used to make decisions based on data. By modelling the algorithms on the bases of historical data, Algorithms find the patterns and relationships that are difficult for humans to detect. These patterns are now further use for the future references to predict solution of unseen problems.\\nSOURCES: https://www.geeksforgeeks.org/machine-learning/\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"total_tokens\": 2114,\n",
      "      \"completion_tokens\": 76,\n",
      "      \"prompt_tokens\": 2038\n",
      "    },\n",
      "    \"model_name\": \"text-davinci-003\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 9:chain:LLMChain] [2.56s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \" Machine learning is used to make decisions based on data. By modelling the algorithms on the bases of historical data, Algorithms find the patterns and relationships that are difficult for humans to detect. These patterns are now further use for the future references to predict solution of unseen problems.\\nSOURCES: https://www.geeksforgeeks.org/machine-learning/\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain] [6.75s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \" Machine learning is used to make decisions based on data. By modelling the algorithms on the bases of historical data, Algorithms find the patterns and relationships that are difficult for humans to detect. These patterns are now further use for the future references to predict solution of unseen problems.\\nSOURCES: https://www.geeksforgeeks.org/machine-learning/\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain] [8.77s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"answer\": \" Machine learning is used to make decisions based on data. By modelling the algorithms on the bases of historical data, Algorithms find the patterns and relationships that are difficult for humans to detect. These patterns are now further use for the future references to predict solution of unseen problems.\\n\",\n",
      "  \"sources\": \"https://www.geeksforgeeks.org/machine-learning/\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': ' Machine learning is used to make decisions based on data. By modelling the algorithms on the bases of historical data, Algorithms find the patterns and relationships that are difficult for humans to detect. These patterns are now further use for the future references to predict solution of unseen problems.\\n',\n",
       " 'sources': 'https://www.geeksforgeeks.org/machine-learning/'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"important questions on ML along with its answer\"\n",
    "\n",
    "\n",
    "langchain.debug=True\n",
    "\n",
    "chain({\"question\": query}, return_only_outputs=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
